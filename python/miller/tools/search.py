"""
Fast semantic and text search tool implementation.

Provides keyword, pattern, semantic, and hybrid search across indexed codebases.
"""

import logging
from typing import Any, Literal, Optional, Union

from miller.tools.search_filters import apply_file_pattern_filter, apply_language_filter

logger = logging.getLogger("miller.search")


async def fast_search(
    query: str,
    method: Literal["auto", "text", "pattern", "semantic", "hybrid"] = "auto",
    limit: int = 20,
    workspace: str = "primary",
    output_format: Literal["text", "json", "toon"] = "text",
    rerank: bool = True,
    expand: bool = False,
    expand_limit: int = 5,
    language: Optional[str] = None,
    file_pattern: Optional[str] = None,
    # These are injected by server.py
    vector_store=None,
    storage=None,
    embeddings=None,
) -> Union[list[dict[str, Any]], str]:
    """
    Search indexed code using text, semantic, or hybrid methods.

    This is the PREFERRED way to find code in the codebase. Use this instead of reading
    files or using grep - semantic search understands what you're looking for!

    When to use: ALWAYS before reading files. Search first to narrow scope by 90%,
    then read only what you need. This is 10x faster than reading entire files.

    You are excellent at crafting search queries. The results are ranked by relevance -
    trust the top results as your answer. No need to verify by reading files -
    Miller's pre-indexed results are accurate and complete.

    Method selection (default: auto):
    - auto: Detects query type automatically (RECOMMENDED)
      * Has special chars (: < > [ ]) â†’ pattern search (code idioms)
      * Natural language â†’ hybrid search (text + semantic)
    - text: Full-text search with stemming (general code search)
    - pattern: Code idioms (: BaseClass, ILogger<, [Fact], etc.)
    - semantic: Vector similarity (conceptual matches)
    - hybrid: Combines text + semantic with RRF fusion

    Output format (default: text):
    - text: Clean, scannable format optimized for AI reading (DEFAULT)
    - json: List of dicts with full metadata (for programmatic use)
    - toon: TOON-formatted string (compact tabular format)

    Examples:
        # Simple search (uses text output by default)
        fast_search("authentication logic")
        fast_search("StorageManager")

        # Method override
        fast_search("user auth", method="semantic")     # Force semantic search
        fast_search(": BaseClass", method="pattern")    # Force pattern search

        # Format override (rarely needed)
        fast_search("auth", output_format="json")   # Get structured data
        fast_search("auth", output_format="toon")   # Get TOON format

        # Workspace-specific search
        fast_search("auth", workspace="my-lib_abc123")

    Args:
        query: Search query (code patterns, keywords, or natural language)
        method: Search method (auto-detects by default)
        limit: Maximum results to return (default: 20)
        workspace: Workspace to query ("primary" or workspace_id from manage_workspace)
        output_format: Output format - "text" (default), "json", or "toon"
        rerank: Enable cross-encoder re-ranking for improved relevance (default: True).
                Adds ~20-50ms latency but improves result quality 15-30%.
                Automatically disabled for pattern search.
        expand: Include caller/callee context for each result (default: False).
                When True, each result includes a 'context' field with direct callers
                and callees. Enables "understanding, not just locations".
        expand_limit: Maximum callers/callees to include per result (default: 5).
        vector_store: VectorStore instance (injected by server)
        storage: StorageManager instance (injected by server)
        embeddings: EmbeddingManager instance (injected by server)

    Returns:
        - text mode: Clean scannable format (name, kind, location, signature)
        - json mode: List of symbol dicts with full metadata
        - toon mode: TOON-formatted string (compact tabular)

    Note: Results are complete and accurate. Trust them - no need to verify with file reads!
    """

    # If workspace specified (and not "primary"), use that workspace's vector store
    # "primary" uses the default injected stores (fast path)
    # Track which storage to use for expansion (workspace-specific or injected)
    active_storage = storage

    # Track workspace-specific resources for cleanup
    workspace_vector_store = None
    workspace_storage = None

    if workspace and workspace != "primary":
        from miller.workspace_paths import get_workspace_vector_path
        from miller.workspace_registry import WorkspaceRegistry

        # Verify workspace exists
        registry = WorkspaceRegistry()
        workspace_entry = registry.get_workspace(workspace)

        if not workspace_entry:
            # Return formatted "no results" for non-existent workspace
            if output_format == "text":
                return f'No matches for "{query}" (workspace "{workspace}" not found).'
            elif output_format == "toon":
                from miller.toon_types import encode_toon
                return encode_toon([])
            else:
                return []

        # Open workspace-specific vector store
        from miller.embeddings import VectorStore

        workspace_vector_path = get_workspace_vector_path(workspace)
        workspace_vector_store = VectorStore(
            db_path=str(workspace_vector_path), embeddings=embeddings
        )

        # Search in workspace-specific store
        results = workspace_vector_store.search(query, method=method, limit=limit)

        # Hydrate with full data from workspace-specific SQLite
        from miller.storage import StorageManager
        from miller.workspace_paths import get_workspace_db_path

        workspace_db_path = get_workspace_db_path(workspace)
        if workspace_db_path.exists():
            workspace_storage = StorageManager(db_path=str(workspace_db_path))
            results = _hydrate_search_results(results, workspace_storage)
            # Use workspace-specific storage for expansion too
            active_storage = workspace_storage
    else:
        # Use default vector store (primary workspace)
        results = vector_store.search(query, method=method, limit=limit)

        # Hydrate with full data from primary workspace SQLite
        if storage is not None:
            results = _hydrate_search_results(results, storage)

    # Re-rank results using cross-encoder (skip for pattern search - exact match)
    # Pattern search uses FTS which already has precise ranking
    if rerank and method != "pattern" and results:
        from miller.reranker import rerank_search_results

        results = rerank_search_results(query, results, enabled=rerank)

    # Apply language filter if specified
    if language is not None:
        results = apply_language_filter(results, language)

    # Apply file pattern filter if specified
    if file_pattern is not None:
        results = apply_file_pattern_filter(results, file_pattern)

    # Semantic fallback: when text search returns poor results, try semantic
    # Triggers when: (1) no results, OR (2) all scores below quality threshold
    # This catches garbage results from searches like "xyznonexistent123"
    original_method = method
    semantic_fallback_used = False
    LOW_SCORE_THRESHOLD = 0.3  # Below this, results are likely irrelevant
    max_score = max((r.get("score", 0.0) for r in results), default=0.0) if results else 0.0
    should_fallback = method == "text" and (not results or max_score < LOW_SCORE_THRESHOLD)
    if should_fallback:
        if not results:
            logger.info("ðŸ”„ Text search returned 0 results, attempting semantic fallback")
        else:
            logger.info(f"ðŸ”„ Text search max score ({max_score:.2f}) below threshold ({LOW_SCORE_THRESHOLD}), attempting semantic fallback")
        # Try semantic search as fallback
        if workspace_vector_store is not None:
            results = workspace_vector_store.search(query, method="semantic", limit=limit)
        else:
            results = vector_store.search(query, method="semantic", limit=limit)

        # Hydrate semantic results
        if results:
            if workspace_storage is not None:
                results = _hydrate_search_results(results, workspace_storage)
            elif storage is not None:
                results = _hydrate_search_results(results, storage)

            # Apply filters to semantic results too
            if language is not None:
                results = apply_language_filter(results, language)
            if file_pattern is not None:
                results = apply_file_pattern_filter(results, file_pattern)

            semantic_fallback_used = True
            logger.info(f"âœ… Semantic fallback found {len(results)} results")

    # Expand results with caller/callee context if requested
    # Use active_storage (workspace-specific or primary) for correct expansion
    if expand and active_storage is not None and results:
        results = _expand_search_results(results, active_storage, expand_limit=expand_limit)

    # Format results for MCP
    formatted = []
    for r in results:
        entry = {
            "id": r.get("id"),  # Needed for tools that work with symbol IDs
            "name": r.get("name", ""),
            "kind": r.get("kind", ""),
            "language": r.get("language", ""),  # Include language for filtering visibility
            "file_path": r.get("file_path", ""),
            "signature": r.get("signature"),
            "doc_comment": r.get("doc_comment"),
            "start_line": r.get("start_line", 0),
            "score": r.get("score", 0.0),
            "code_context": r.get("code_context"),  # For grep-style output
        }
        # Include context if expansion was enabled
        if expand and "context" in r:
            entry["context"] = r["context"]
        formatted.append(entry)

    # Cleanup workspace-specific resources before returning
    # These are only created for non-primary workspace searches
    def _cleanup():
        if workspace_storage is not None:
            workspace_storage.close()
        if workspace_vector_store is not None:
            workspace_vector_store.close()

    # Apply output format
    if output_format == "text":
        result = _format_search_as_text(formatted, query=query)
        # Add semantic fallback notice if used
        if semantic_fallback_used and formatted:
            fallback_notice = "ðŸ”„ Text search returned 0 results. Showing semantic matches instead.\nðŸ’¡ Semantic search finds conceptually similar code even when exact terms don't match.\n\n"
            result = fallback_notice + result
    elif output_format == "toon":
        from miller.toon_types import encode_toon
        result = encode_toon(formatted)
    else:  # json
        result = formatted

    _cleanup()
    return result


def _hydrate_search_results(
    search_results: list[dict[str, Any]], storage: "StorageManager"
) -> list[dict[str, Any]]:
    """Hydrate search results with full symbol data from SQLite.

    OPTIMIZED: Uses single batch query instead of N individual queries.
    This reduces ~20 queries to 1 for typical search results.

    Vector search returns lean results (id, name, kind, score).
    This function enriches them with full data from SQLite,
    including code_context for grep-style output.

    Args:
        search_results: Lean results from vector search
        storage: StorageManager instance for SQLite lookups

    Returns:
        Hydrated results with code_context and other fields from SQLite.
        Preserves search score (not storage score).
    """
    if not search_results:
        return []

    # Collect all symbol IDs and their scores in one pass
    symbol_ids = []
    score_map = {}  # id -> search_score
    for result in search_results:
        symbol_id = result.get("id")
        if symbol_id:
            symbol_ids.append(symbol_id)
            score_map[symbol_id] = result.get("score", 0.0)

    # Single batch query instead of N queries!
    symbols_map = storage.get_symbols_by_ids(symbol_ids) if symbol_ids else {}

    # Build hydrated results, preserving order
    hydrated = []
    for result in search_results:
        symbol_id = result.get("id")

        if symbol_id and symbol_id in symbols_map:
            # Use full symbol data from batch lookup
            full_symbol = symbols_map[symbol_id].copy()
            full_symbol["score"] = score_map[symbol_id]
            hydrated.append(full_symbol)
        else:
            # Fallback: keep original result if hydration fails
            hydrated.append(result)

    return hydrated


def _expand_search_results(
    results: list[dict[str, Any]],
    storage: "StorageManager",
    expand_limit: int = 5,
) -> list[dict[str, Any]]:
    """Expand search results with caller/callee context.

    OPTIMIZED: Uses batch queries instead of N+1 pattern.
    Reduces ~240 queries to ~3 queries for 20 results with expand=True.

    For each search result, adds a 'context' field containing:
    - callers: Direct callers of this symbol (distance=1 from reachability)
    - callees: Direct callees of this symbol (distance=1)
    - caller_count: Total number of callers (may be > len(callers) due to limit)
    - callee_count: Total number of callees

    This enables "understanding, not just locations" - when you find a symbol,
    you immediately see who uses it and what it depends on.

    Args:
        results: List of search result dicts (must have 'id' field)
        storage: StorageManager instance for lookups
        expand_limit: Max callers/callees to include per symbol (default 5)

    Returns:
        Results with added 'context' field for each entry.
    """
    if not results:
        return []

    # Collect all symbol IDs that have valid IDs
    symbol_ids = [r.get("id") for r in results if r.get("id")]

    if not symbol_ids:
        # No valid IDs, just add empty context to all
        for result in results:
            result["context"] = {
                "callers": [],
                "callees": [],
                "caller_count": 0,
                "callee_count": 0,
            }
        return results

    # BATCH QUERY 1: Get all callers for all symbols (single query!)
    callers_map = storage.get_reachability_for_targets_batch(symbol_ids, min_distance=1)

    # BATCH QUERY 2: Get all callees for all symbols (single query!)
    callees_map = storage.get_reachability_from_sources_batch(symbol_ids, min_distance=1)

    # Collect all unique caller/callee IDs that need hydration
    all_related_ids: set[str] = set()
    for symbol_id in symbol_ids:
        callers = callers_map.get(symbol_id, [])
        callees = callees_map.get(symbol_id, [])
        # Only collect IDs we'll actually use (up to expand_limit per symbol)
        for c in callers[:expand_limit]:
            if c.get("source_id"):
                all_related_ids.add(c["source_id"])
        for c in callees[:expand_limit]:
            if c.get("target_id"):
                all_related_ids.add(c["target_id"])

    # BATCH QUERY 3: Hydrate all caller/callee symbols (single query!)
    symbols_map = storage.get_symbols_by_ids(list(all_related_ids)) if all_related_ids else {}

    # Build expanded results
    expanded = []
    for result in results:
        symbol_id = result.get("id")

        if not symbol_id:
            # Can't expand without ID - keep original
            result["context"] = {
                "callers": [],
                "callees": [],
                "caller_count": 0,
                "callee_count": 0,
            }
            expanded.append(result)
            continue

        # Get callers and callees from batch results
        direct_callers = callers_map.get(symbol_id, [])
        direct_callees = callees_map.get(symbol_id, [])

        # Build caller details from pre-fetched symbols
        caller_details = []
        for caller in direct_callers[:expand_limit]:
            caller_id = caller.get("source_id")
            if caller_id and caller_id in symbols_map:
                sym = symbols_map[caller_id]
                caller_details.append({
                    "id": caller_id,
                    "name": sym.get("name", "?"),
                    "kind": sym.get("kind", "?"),
                    "file_path": sym.get("file_path", "?"),
                    "line": sym.get("start_line", 0),
                })

        # Build callee details from pre-fetched symbols
        callee_details = []
        for callee in direct_callees[:expand_limit]:
            callee_id = callee.get("target_id")
            if callee_id and callee_id in symbols_map:
                sym = symbols_map[callee_id]
                callee_details.append({
                    "id": callee_id,
                    "name": sym.get("name", "?"),
                    "kind": sym.get("kind", "?"),
                    "file_path": sym.get("file_path", "?"),
                    "line": sym.get("start_line", 0),
                })

        # Add context to result
        result["context"] = {
            "callers": caller_details,
            "callees": callee_details,
            "caller_count": len(direct_callers),
            "callee_count": len(direct_callees),
        }
        expanded.append(result)

    return expanded


def _format_search_as_text(results: list[dict[str, Any]], query: str = "") -> str:
    """Format search results as grep-style text output.

    Output format (inspired by Julie's lean format):
    ```
    N matches for "query":

    src/file.py:42
      41: # context before
      42â†’ def matched_line():
      43:     # context after

    src/other.py:100
      99: # context
      100â†’ another_match
    ```

    Benefits over JSON/TOON:
    - 80% fewer tokens than JSON
    - 60% fewer tokens than TOON
    - Zero parsing overhead - just read the text
    - Grep-style output familiar to developers

    Args:
        results: List of search result dicts with file_path, start_line, code_context
        query: The search query (for header)

    Returns:
        Grep-style formatted text string
    """
    if not results:
        return f'No matches for "{query}".' if query else "No results found."

    output = []

    # Header with count and query
    count = len(results)
    if query:
        match_word = "match" if count == 1 else "matches"
        output.append(f'{count} {match_word} for "{query}":')
    else:
        output.append(f"{count} results:")
    output.append("")  # Blank line after header

    # Each result: file:line header + indented code context
    for r in results:
        file_path = r.get("file_path", "?")
        start_line = r.get("start_line", 0)
        code_context = r.get("code_context")
        signature = r.get("signature", "")

        # File:line header
        output.append(f"{file_path}:{start_line}")

        # Indented code context (preferred) or signature (fallback) or name (last resort)
        if code_context:
            for line in code_context.split("\n"):
                output.append(f"  {line}")
        elif signature:
            # Fallback to signature if no code_context
            output.append(f"  {signature}")
        else:
            # Last resort: show name (kind) so result isn't empty/useless
            name = r.get("name", "?")
            kind = r.get("kind", "symbol")
            output.append(f"  {name} ({kind})")

        # Context information (when expand=True)
        context = r.get("context")
        if context:
            callers = context.get("callers", [])
            callees = context.get("callees", [])
            caller_count = context.get("caller_count", 0)
            callee_count = context.get("callee_count", 0)

            if caller_count > 0:
                caller_strs = [f"{c['name']} ({c['file_path']}:{c['line']})" for c in callers]
                more = f" +{caller_count - len(callers)} more" if caller_count > len(callers) else ""
                output.append(f"  â† Callers ({caller_count}): {', '.join(caller_strs)}{more}")

            if callee_count > 0:
                callee_strs = [f"{c['name']} ({c['file_path']}:{c['line']})" for c in callees]
                more = f" +{callee_count - len(callees)} more" if callee_count > len(callees) else ""
                output.append(f"  â†’ Callees ({callee_count}): {', '.join(callee_strs)}{more}")

        output.append("")  # Blank line between results

    # Trim trailing blank line
    while output and output[-1] == "":
        output.pop()

    return "\n".join(output)
