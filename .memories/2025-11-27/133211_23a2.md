---
git:
  branch: main
  commit: 17b3949
  dirty: true
  files_changed:
  - python/miller/embeddings/fts_index.py
  - python/miller/embeddings/manager.py
  - python/miller/embeddings/search_enhancements.py
  - python/miller/embeddings/search_methods.py
  - python/miller/embeddings/vector_store.py
  - python/miller/ignore_defaults.py
  - python/miller/workspace/indexer.py
  - python/miller/workspace/scanner.py
  - src/bindings/api.rs
  - nul
  - python/tests/test_file_batch_sizing.py
  - python/tests/test_file_level_indexing.py
id: checkpoint_9cc5acb1_1a635b
tags:
- gpu
- batch-size
- performance
- directml
- cuda
- vram
timestamp: 1764271931
type: checkpoint
---

Implemented VRAM-based file batch sizing: DirectML gets conservative 15 files/batch (fragile integrated GPUs), CPU gets 50 (I/O bound), CUDA/MPS/XPU scale 25-100 based on VRAM. Fixed hardcoded BATCH_SIZE=10 in scanner.py that was throttling 12GB NVIDIA to 1.8GB usage. Added 12 tests covering all device types and edge cases.
